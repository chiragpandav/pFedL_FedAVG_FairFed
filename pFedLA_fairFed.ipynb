{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f3f2206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef46b949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a50eedd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "from argparse import Namespace\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "from sklearn import preprocessing\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from folktables import ACSDataSource, ACSEmployment,ACSIncome\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from typing import Dict, List, OrderedDict, Tuple\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "from typing import OrderedDict\n",
    "from typing import Dict, List, OrderedDict, Tuple, Union\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import random\n",
    "from typing import Union\n",
    "\n",
    "from path import Path\n",
    "from rich.console import Console\n",
    "from rich.progress import track\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "from typing import List, OrderedDict, Tuple\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa7fcf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acd50dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted: /home/chiragapandav/Downloads/Hiwi/Improving-Fairness-via-Federated-Learning/FedFB/my_model/epoch.pkl\n",
      "Deleted: /home/chiragapandav/Downloads/Hiwi/Improving-Fairness-via-Federated-Learning/FedFB/my_model/clients_model.pt\n",
      "Deleted: /home/chiragapandav/Downloads/Hiwi/Improving-Fairness-via-Federated-Learning/FedFB/hn/0.pkl\n",
      "Deleted: /home/chiragapandav/Downloads/Hiwi/Improving-Fairness-via-Federated-Learning/FedFB/hn/3.pkl\n",
      "Deleted: /home/chiragapandav/Downloads/Hiwi/Improving-Fairness-via-Federated-Learning/FedFB/hn/2.pkl\n",
      "Deleted: /home/chiragapandav/Downloads/Hiwi/Improving-Fairness-via-Federated-Learning/FedFB/hn/1.pkl\n",
      "All files removed successfully.\n"
     ]
    }
   ],
   "source": [
    "old_model_path = \"/home/chiragapandav/Downloads/Hiwi/Improving-Fairness-via-Federated-Learning/FedFB/my_model/\"\n",
    "client_models_path = \"/home/chiragapandav/Downloads/Hiwi/Improving-Fairness-via-Federated-Learning/FedFB/hn\"\n",
    "\n",
    "\n",
    "for file in os.listdir(old_model_path):\n",
    "    file_path = os.path.join(old_model_path, file)\n",
    "    os.remove(file_path)\n",
    "    print(f\"Deleted: {file_path}\")\n",
    "    \n",
    "for file in os.listdir(client_models_path):\n",
    "    file_path = os.path.join(client_models_path, file)\n",
    "    os.remove(file_path)\n",
    "    print(f\"Deleted: {file_path}\")\n",
    "        \n",
    "\n",
    "\n",
    "print(\"All files removed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc33154",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad00a80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fix_random_seed(seed: int) -> None:\n",
    "    \n",
    "#     print(\"fix_random_seed Chirag\")\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    torch.random.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    \n",
    "def clone_parameters(\n",
    "    src: Union[OrderedDict[str, torch.Tensor], torch.nn.Module]\n",
    ") -> OrderedDict[str, torch.Tensor]:\n",
    "    \n",
    "#     print(\"clone_parameters Chirag\")\n",
    "    \n",
    "    if isinstance(src, OrderedDict):\n",
    "        return OrderedDict(\n",
    "            {\n",
    "                name: param.clone().detach().requires_grad_(param.requires_grad)\n",
    "                for name, param in src.items()\n",
    "            }\n",
    "        )\n",
    "    if isinstance(src, torch.nn.Module):\n",
    "        return OrderedDict(\n",
    "            {\n",
    "                name: param.clone().detach().requires_grad_(param.requires_grad)\n",
    "                for name, param in src.state_dict(keep_vars=True).items()\n",
    "            }\n",
    "        )\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5d42ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 14 : input shape\n",
    "        self.layer1 = nn.Linear(14, 512)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "        self.layer2 = nn.Linear(512, 256)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(256, 60)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.output = nn.Linear(60, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.layer1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.act2(self.layer2(x))\n",
    "        x = self.act3(self.layer3(x))\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e905b8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ClientBase:\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone: torch.nn.Module,\n",
    "        dataset: str,\n",
    "        batch_size: int,\n",
    "        valset_ratio: float,\n",
    "        testset_ratio: float,\n",
    "        local_epochs: int,\n",
    "        local_lr: float,\n",
    "        logger: Console,\n",
    "        gpu: int,\n",
    "    ):\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if gpu and torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        self.client_id: int = None\n",
    "        self.valset: DataLoader = None\n",
    "        self.trainset: DataLoader = None\n",
    "        self.testset: DataLoader = None\n",
    "        \n",
    "        \n",
    "        # need to change\n",
    "#         self.model: torch.nn.Module = init_model\n",
    "        self.model: torch.nn.Module = deepcopy(backbone)\n",
    "        \n",
    "#         print(\"model client\", self.model.parameters())\n",
    "        \n",
    "#         self.optimizer: torch.optim.Optimizer = torch.optim.SGD(\n",
    "#             self.model.parameters(), lr=local_lr\n",
    "#         )\n",
    "            \n",
    "        self.optimizer: torch.optim.Optimizer = torch.optim.Adam(\n",
    "            self.model.parameters(), lr=local_lr\n",
    "        )\n",
    "          \n",
    "        self.batch_size = batch_size\n",
    "        self.valset_ratio = valset_ratio\n",
    "        self.testset_ratio = testset_ratio\n",
    "        self.local_epochs = local_epochs\n",
    "        self.local_lr = local_lr\n",
    "        self.criterion = torch.nn.BCELoss()\n",
    "        self.logger = logger\n",
    "        \n",
    "        print(\"Base Client constructor\")\n",
    "\n",
    "   \n",
    "    @torch.no_grad()\n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        size = 0\n",
    "        loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, targets,sens in self.testset:\n",
    "            \n",
    "            inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "            \n",
    "            outputs = self.model(inputs)\n",
    "            loss += self.criterion(outputs, targets)\n",
    "            predicted = outputs > 0.5\n",
    "            \n",
    "            \n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            \n",
    "#             acc += (y_pred.round() == y).float().mean()\n",
    "        \n",
    "                \n",
    "#         print(\"Correct: \",correct,\"Total: \", total)\n",
    "        loss = loss / len(self.testset)\n",
    "        acc = correct / total\n",
    " \n",
    "#         print(\"loss: %f\\n\" % (loss))\n",
    "        print(f\"Testing Accuracy: {acc:.2%}\")\n",
    "        \n",
    "        return loss, acc\n",
    "    \n",
    "\n",
    "    def train(self):\n",
    "        print(\"Base Client train called\")\n",
    "        pass\n",
    "\n",
    "    def _train(self):\n",
    "        print(\"Base Client __train called\")\n",
    "        pass\n",
    "\n",
    "    def get_client_local_dataset(self):\n",
    "        \n",
    "#         datasets = get_dataloader(\n",
    "#             self.dataset,\n",
    "#             self.client_id,\n",
    "#             self.batch_size,\n",
    "#             self.valset_ratio,\n",
    "#             self.testset_ratio,\n",
    "#         )\n",
    "        data_dir=\"/home/chiragapandav/Downloads/Hiwi/Improving-Fairness-via-Federated-Learning/FedFB/data_fairFed\"\n",
    "        with open( data_dir+\"/clients_training.pkl\", \"rb\") as f:\n",
    "            self.trainset = pickle.load(f)\n",
    "            \n",
    "        with open(data_dir+\"/clients_validation.pkl\", \"rb\") as f:\n",
    "            self.valset = pickle.load(f)\n",
    "         \n",
    "        with open(data_dir+\"/clients_testing_wrong.pkl\", \"rb\") as f:\n",
    "            self.testset  = pickle.load(f)\n",
    "\n",
    "        self.trainset = self.trainset[self.client_id]\n",
    "        self.valset = self.valset[self.client_id]       \n",
    "        self.testset = self.testset[self.client_id]\n",
    "\n",
    "        print(\"self.client_id:: \",self.client_id)\n",
    "                                \n",
    "\n",
    "    def _log_while_training(self, evaluate=True, verbose=False):\n",
    "        def _log_and_train(*args, **kwargs):\n",
    "            loss_before = 0\n",
    "            loss_after = 0\n",
    "            acc_before = 0\n",
    "            acc_after = 0\n",
    "            if evaluate:\n",
    "                loss_before, acc_before = self.evaluate()\n",
    "\n",
    "            res = self._train(*args, **kwargs)\n",
    "\n",
    "            if evaluate:\n",
    "                loss_after, acc_after = self.evaluate()\n",
    "            \n",
    "            \n",
    "#             if verbose:\n",
    "#                 self.logger.log(\n",
    "#                     \"client [{}]   [bold red]loss: {:.4f} -> {:.4f}    [bold blue]accuracy: {:.2f}% -> {:.2f}%\".format(\n",
    "#                         self.client_id, loss_before, loss_after, acc_before, acc_after\n",
    "#                     )\n",
    "#                 )\n",
    "\n",
    "            eval_stats = {\n",
    "                \"loss_before\": loss_before,\n",
    "                \"loss_after\": loss_after,\n",
    "                \"acc_before\": acc_before,\n",
    "                \"acc_after\": acc_after,\n",
    "            }\n",
    "            \n",
    "            return res, eval_stats\n",
    "\n",
    "        return _log_and_train\n",
    "\n",
    "    def set_parameters(self, model_params: OrderedDict):\n",
    "\n",
    "        self.model.load_state_dict(model_params, strict=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cde0850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5055145d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class pFedLAClient(ClientBase):\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone: torch.nn.Module,        \n",
    "        dataset: str,\n",
    "        batch_size: int,\n",
    "        valset_ratio: float,\n",
    "        testset_ratio: float,\n",
    "        local_epochs: int,\n",
    "        local_lr: float,\n",
    "        logger: Console,\n",
    "        gpu: int,\n",
    "    ):\n",
    "        super(pFedLAClient, self).__init__(\n",
    "            backbone,\n",
    "            dataset,\n",
    "            batch_size,\n",
    "            valset_ratio,\n",
    "            testset_ratio,\n",
    "            local_epochs,\n",
    "            logger,\n",
    "            local_lr,\n",
    "            gpu,\n",
    "        )\n",
    "        print(\"PFedLa Client constructor\")\n",
    "        self.all_client_info=[]\n",
    "    \n",
    "    def train(\n",
    "        self,\n",
    "        client_id: int,\n",
    "        model_params: OrderedDict[str, torch.Tensor],\n",
    "        verbose=True,\n",
    "    ): \n",
    "#         print(\"pFedL Client train called\")\n",
    "        self.client_id = client_id\n",
    "        \n",
    "#         model_params= DeepNet()\n",
    "\n",
    "        self.set_parameters(model_params)                \n",
    "        self.get_client_local_dataset()\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        res, stats = self._log_while_training(evaluate=True, verbose=verbose)()\n",
    "        self.model.cpu()\n",
    "        \n",
    "        self.all_client_info.append(res)\n",
    "        return res, stats\n",
    "        \n",
    "    def _train(self):\n",
    "#         print(\"pFedL Client __train called\")\n",
    "        self.model.train()\n",
    "        frz_model_params = clone_parameters(self.model)\n",
    "\n",
    "        for epoch in range(self.local_epochs):\n",
    "            print(epoch)\n",
    "            for x, y,z in self.trainset:\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "\n",
    "                logits = self.model(x)\n",
    "\n",
    "                loss = self.criterion(logits, y)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "        delta = OrderedDict(\n",
    "            {\n",
    "                k: p1 - p0 \n",
    "                for (k, p1), p0 in zip(\n",
    "                    self.model.state_dict(keep_vars=True).items(),\n",
    "                    frz_model_params.values(),\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return delta\n",
    "    \n",
    "    def test(\n",
    "        self, client_id: int, model_params: OrderedDict[str, torch.Tensor],\n",
    "    ):\n",
    "        self.client_id = client_id\n",
    "        self.set_parameters(model_params)\n",
    "        self.get_client_local_dataset()\n",
    "        self.model.to(self.device)\n",
    "        loss, acc = self.evaluate()\n",
    "        dummy_diff = OrderedDict(\n",
    "            {\n",
    "                name: torch.zeros_like(param)\n",
    "                for name, param in self.model.state_dict().items()\n",
    "            }\n",
    "        )\n",
    "        self.model.cpu()\n",
    "        stats = {\"loss\": loss, \"acc\": acc}\n",
    "        return dummy_diff, stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2499dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5aff4856",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_features, out_features) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.bias = nn.Parameter(torch.empty(out_features))\n",
    "\n",
    "        nn.init.uniform_(self.weight)\n",
    "        nn.init.constant_(self.bias, 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.linear(x, self.weight, self.bias)\n",
    "    \n",
    "\n",
    "class HyperNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        hidden_dim: int,\n",
    "        backbone: nn.Module,\n",
    "        client_num: int,\n",
    "        K: int,\n",
    "        gpu=True,\n",
    "    ):\n",
    "    \n",
    "        super(HyperNetwork, self).__init__()\n",
    "        self.device = torch.device(\n",
    "                \"cuda\" if gpu and torch.cuda.is_available() else \"cpu\"\n",
    "            )\n",
    "        \n",
    "        self.K = K\n",
    "        self.client_num = client_num\n",
    "        self.embedding = nn.Embedding(client_num, embedding_dim, device=self.device)\n",
    "        self.blocks_name = set(n.split(\".\")[0] for n, _ in backbone.named_parameters())\n",
    "        self.cache_dir =  \"/home/chiragapandav/Downloads/Hiwi/Improving-Fairness-via-Federated-Learning/FedFB/hn/\"  # put dir here\n",
    "        \n",
    "        if os.listdir(self.cache_dir) != client_num:\n",
    "            \n",
    "            for client_id in range(client_num):\n",
    "#                 with open(self.cache_dir / f\"{client_id}.pkl\", \"wb\") as f:\n",
    "                with open(os.path.join(self.cache_dir, f\"{client_id}.pkl\"), \"wb\") as f:\n",
    "\n",
    "                    pickle.dump(\n",
    "                        {\n",
    "                            \"mlp\": nn.Sequential(\n",
    "                                nn.Linear(embedding_dim, hidden_dim),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(hidden_dim, hidden_dim),\n",
    "                                nn.ReLU(),\n",
    "                            ),\n",
    "                            \n",
    "                            \"fc\": {\n",
    "                                name: Linear(hidden_dim, client_num)\n",
    "                                for name in self.blocks_name\n",
    "                            },\n",
    "                        },\n",
    "                        f,\n",
    "                    )\n",
    "\n",
    "        # for tracking the current client's hn parameters\n",
    "        self.current_client_id: int = None\n",
    "        self.mlp: nn.Sequential = None\n",
    "        self.fc_layers: Dict[str, Linear] = {}\n",
    "        self.retain_blocks: List[str] = []\n",
    "            \n",
    "#         print(\"HypterNetwork\")\n",
    "\n",
    "    def mlp_parameters(self) -> List[nn.Parameter]:\n",
    "            return list(filter(lambda p: p.requires_grad, self.mlp.parameters()))           \n",
    "        \n",
    "    def fc_layer_parameters(self) -> List[nn.Parameter]:\n",
    "        params_list = []\n",
    "        for block, fc in self.fc_layers.items():\n",
    "            if block not in self.retain_blocks:\n",
    "                params_list += list(filter(lambda p: p.requires_grad, fc.parameters()))                \n",
    "        return params_list\n",
    "\n",
    "    def emd_parameters(self) -> List[nn.Parameter]:\n",
    "        return list(self.embedding.parameters())        \n",
    "    \n",
    "    def forward(self, client_id: int) -> Tuple[Dict[str, torch.Tensor], List[str]]:\n",
    "        self.current_client_id = client_id\n",
    "\n",
    "        self.retain_blocks = []\n",
    "        \n",
    "        emd = self.embedding(\n",
    "            torch.tensor(client_id, dtype=torch.long, device=self.device)\n",
    "        )\n",
    "        self.load_hn()\n",
    "\n",
    "        feature = self.mlp(emd)\n",
    "\n",
    "        alpha = {\n",
    "            block: F.relu(self.fc_layers[block](feature)) for block in self.blocks_name\n",
    "        }        \n",
    "\n",
    "        default_weight = torch.tensor(\n",
    "            [i == client_id for i in range(self.client_num)],\n",
    "            dtype=torch.float,\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "        if self.K > 0:  # HeurpFedLA\n",
    "            \n",
    "            blocks_name = []\n",
    "            self_weights = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for name, weight in alpha.items():\n",
    "\n",
    "                    blocks_name.append(name)\n",
    "                    self_weights.append(weight[client_id])\n",
    "\n",
    "                # not in the Loop\n",
    "                _, topk_weights_idx = torch.topk(torch.tensor(self_weights), self.K)\n",
    "                \n",
    "            for i in topk_weights_idx:\n",
    "                alpha[blocks_name[i]] = default_weight\n",
    "                self.retain_blocks.append(blocks_name[i])\n",
    "#             print(\"  alpha: \",alpha,\"\\n\")\n",
    "        return alpha, self.retain_blocks\n",
    "        \n",
    "    \n",
    "    def save_hn(self):\n",
    "            for block, param in self.fc_layers.items():\n",
    "                self.fc_layers[block] = param.cpu()\n",
    "                \n",
    "            file_path = os.path.join(self.cache_dir, f\"{self.current_client_id}.pkl\")\n",
    "            with open(file_path, \"wb\") as f:\n",
    "#             with open(self.cache_dir / f\"{self.current_client_id}.pkl\", \"wb\") as f:\n",
    "                pickle.dump(\n",
    "                    {\"mlp\": self.mlp.cpu(), \"fc\": self.fc_layers}, f,\n",
    "                )\n",
    "            self.mlp = None\n",
    "            self.fc_layers = {}\n",
    "            self.current_client_id = None\n",
    "\n",
    "    def load_hn(self) -> Tuple[nn.Sequential, OrderedDict[str, Linear]]:\n",
    "        with open(os.path.join(self.cache_dir, f\"{self.current_client_id}.pkl\"), \"rb\") as f:\n",
    "        \n",
    "#         with open(self.cache_dir / f\"{self.current_client_id}.pkl\", \"rb\") as f:\n",
    "            parameters = pickle.load(f)\n",
    "        self.mlp = parameters[\"mlp\"].to(self.device)\n",
    "        for block, param in parameters[\"fc\"].items():\n",
    "            self.fc_layers[block] = param.to(self.device)\n",
    "\n",
    "    def clean_models(self):\n",
    "        if os.path.isdir(self.cache_dir):\n",
    "            os.system(f\"rm -rf {self.cache_dir}\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39880990",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0d78bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ServerBase:\n",
    "    def __init__(self, args: Namespace, algo: str):\n",
    "        self.algo = algo\n",
    "        self.args = args\n",
    "        \n",
    "        # default log file format\n",
    "        self.log_name = \"{}_{}_{}_{}.html\".format(\n",
    "            self.algo,\n",
    "            self.args.dataset,\n",
    "            self.args.global_epochs,\n",
    "            self.args.local_epochs,\n",
    "        )\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        fix_random_seed(self.args.seed)\n",
    "        \n",
    "#         self.backbone= (DeepNet)   \n",
    "        \n",
    "        self.backbone = (\n",
    "            CNNWithBatchNorm\n",
    "            if self.args.dataset in [\"cifar10\", \"cifar100\"]\n",
    "            else DeepNet\n",
    "        )\n",
    "        \n",
    "        print(\"self.backbone\",self.backbone)\n",
    "        \n",
    "        self.logger = Console(record=True, log_path=False, log_time=False,)\n",
    "        \n",
    "        self.client_id_indices, self.client_num_in_total = [0,1,2,3], 4                \n",
    "        \n",
    "        self.temp_dir =\"/home/chiragapandav/Downloads/Hiwi/Improving-Fairness-via-Federated-Learning/FedFB//my_model/\"\n",
    "\n",
    "#         name_of_model = '../../WW_WM_BW.pth'\n",
    "        #  self.temp_dir = TEMP_DIR / self.algo\n",
    "        if not os.path.isdir(self.temp_dir):\n",
    "            os.makedirs(self.temp_dir)\n",
    "            \n",
    "#         _dummy_model = self.backbone(self.args.dataset)\n",
    "        _dummy_model = self.backbone()\n",
    "#         _dummy_model = DeepNet()\n",
    "    \n",
    "#         print(\"_dummy_model:: base\",_dummy_model)\n",
    "        \n",
    "        passed_epoch = 0\n",
    "        self.global_params_dict: OrderedDict[str : torch.Tensor] = None\n",
    "        \n",
    "        if os.listdir(self.temp_dir) != []:\n",
    "            if os.path.exists(os.path.join(self.temp_dir, \"global_model.pt\")):\n",
    "                \n",
    "                self.global_params_dict = torch.load(self.temp_dir / \"global_model.pt\")\n",
    "                self.logger.log(\"Find existed global model...\")\n",
    "\n",
    "            if os.path.exists(os.path.join(self.temp_dir, \"epoch.pkl\")):  \n",
    "                with open(os.path.join(self.temp_dir, \"epoch.pkl\"), \"rb\") as f:\n",
    "#               \n",
    "                    passed_epoch = pickle.load(f)\n",
    "                self.logger.log(f\"Have run {passed_epoch} epochs already.\",)\n",
    "        else:\n",
    "            self.global_params_dict = OrderedDict(_dummy_model.state_dict())\n",
    "            \n",
    "#         self.global_params_dict = OrderedDict(_dummy_model.state_dict())\n",
    "#         print(\"self.global_params_dict: \",self.global_params_dict)\n",
    "\n",
    "        self.global_epochs = self.args.global_epochs - passed_epoch\n",
    "#         self.global_epochs = self.global_epochs - passed_epoch\n",
    "    \n",
    "        self.logger.log(\"Backbone:\", _dummy_model)\n",
    "\n",
    "        self.trainer: ClientBase = None\n",
    "        self.all_clients_stats = {i: {} for i in self.client_id_indices}\n",
    "           \n",
    "            \n",
    "        print(\"Base Server Called !\")\n",
    "        \n",
    "    def train(self):\n",
    "\n",
    "        print(\"In Base server  train\\n\")\n",
    "        \n",
    "        self.logger.log(\"=\" * 30, \"TRAINING\", \"=\" * 30, style=\"bold green\")\n",
    "        progress_bar = (\n",
    "            track(\n",
    "                range(self.global_epochs),\n",
    "                \"[bold green]Training...\",\n",
    "                console=self.logger,\n",
    "            )\n",
    "            if not self.args.log\n",
    "            else tqdm(range(self.global_epochs), \"Training...\")\n",
    "        )\n",
    "        for E in progress_bar:\n",
    "\n",
    "            if E % self.args.verbose_gap == 0:\n",
    "                self.logger.log(\"=\" * 30, f\"ROUND: {E}\", \"=\" * 30)\n",
    "\n",
    "            selected_clients = random.sample(\n",
    "                self.client_id_indices, self.args.client_num_per_round\n",
    "            )\n",
    "            \n",
    "            updated_params_cache = []\n",
    "            weights_cache = []\n",
    "\n",
    "            for client_id in selected_clients:\n",
    "                \n",
    "                client_local_params = clone_parameters(self.global_params_dict)\n",
    "                                \n",
    "                (updated_params, weight), stats = self.trainer.train(\n",
    "                    client_id=client_id,\n",
    "                    model_params=client_local_params,\n",
    "                    verbose=(E % self.args.verbose_gap) == 0,\n",
    "                )\n",
    "\n",
    "                updated_params_cache.append(updated_params)\n",
    "                weights_cache.append(weight)\n",
    "                self.all_clients_stats[client_id][f\"ROUND: {E}\"] = (\n",
    "                    f\"{stats['loss_before']:.4f} -> {stats['loss_after']:.4f}\",\n",
    "                )\n",
    "\n",
    "            self.aggregate_parameters(updated_params_cache, weights_cache)\n",
    "\n",
    "            if E % self.args.save_period == 0:\n",
    "                torch.save(\n",
    "                    self.global_params_dict, self.temp_dir / \"global_model.pt\",\n",
    "                )\n",
    "                with open(self.temp_dir / \"epoch.pkl\", \"wb\") as f:\n",
    "                    pickle.dump(E, f)\n",
    "        self.logger.log(self.all_clients_stats)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def aggregate_parameters(self, updated_params_cache, weights_cache):\n",
    "        weight_sum = sum(weights_cache)\n",
    "        weights = torch.tensor(weights_cache, device=self.device) / weight_sum\n",
    "\n",
    "        aggregated_params = []\n",
    "\n",
    "        for params in zip(*updated_params_cache):\n",
    "            aggregated_params.append(\n",
    "                torch.sum(weights * torch.stack(params, dim=-1), dim=-1)\n",
    "            )\n",
    "\n",
    "        self.global_params_dict = OrderedDict(\n",
    "            zip(self.global_params_dict.keys(), aggregated_params)\n",
    "        )\n",
    "\n",
    "                \n",
    "    def test(self) -> None:\n",
    "        self.logger.log(\"=\" * 30, \"TESTING\", \"=\" * 30, style=\"bold blue\")\n",
    "        all_loss = []\n",
    "        all_acc = []\n",
    "        for client_id in track(\n",
    "            self.client_id_indices,\n",
    "            \"[bold blue]Testing...\",\n",
    "            console=self.logger,\n",
    "            disable=self.args.log,\n",
    "        ):\n",
    "            client_local_params = clone_parameters(self.global_params_dict)\n",
    "            stats = self.trainer.test(\n",
    "                client_id=client_id, model_params=client_local_params,\n",
    "            )\n",
    "\n",
    "            self.logger.log(\n",
    "                f\"client [{client_id}]  [red]loss: {stats['loss']:.4f}    [magenta]accuracy: {stats['acc']:.2f}%\"\n",
    "            )\n",
    "            all_loss.append(stats[\"loss\"])\n",
    "            all_acc.append(stats[\"acc\"])\n",
    "\n",
    "        self.logger.log(\"=\" * 20, \"RESULTS\", \"=\" * 20, style=\"bold green\")\n",
    "        self.logger.log(\n",
    "            \"loss: {:.4f}    accuracy: {:.2f}%\".format(\n",
    "                sum(all_loss) / len(all_loss), sum(all_acc) / len(all_acc),\n",
    "            )\n",
    "        )        \n",
    "    \n",
    "    def run(self):\n",
    "        print(\"Base server Run called\")\n",
    "        self.logger.log(\"Arguments:\", dict(self.args._get_kwargs()))\n",
    "        self.train()\n",
    "        self.test()\n",
    "        if self.args.log:\n",
    "            if not os.path.isdir(LOG_DIR):\n",
    "                os.mkdir(LOG_DIR)\n",
    "            self.logger.save_html(LOG_DIR / self.log_name)\n",
    "\n",
    "        # delete all temporary files\n",
    "#         if os.listdir(self.temp_dir) != []:\n",
    "#             os.system(f\"rm -rf {self.temp_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610b1470",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2972ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class pFedLAServer(ServerBase):\n",
    "    def __init__(self):\n",
    "        super(pFedLAServer, self).__init__(args1, \"pFedLA\")\n",
    "        \n",
    "        self.log_name = \"{}_{}_{}_{}_{}.html\".format(\n",
    "            self.algo,\n",
    "            self.args.dataset,\n",
    "            self.args.global_epochs,\n",
    "            self.args.local_epochs,\n",
    "            self.args.k,\n",
    "        )\n",
    "        \n",
    "        self.tracker_temp=0\n",
    "        self.tracker_diff=[]\n",
    "        \n",
    "        if self.global_params_dict is not None:\n",
    "            del self.global_params_dict  # pFedLA don't have global model\n",
    "\n",
    "        if os.listdir(self.temp_dir) != []:             \n",
    "            if os.path.exists(os.path.join(self.temp_dir, \"clients_model.pt\")):\n",
    "                self.client_model_params_list = torch.load(\n",
    "                    os.path.join(self.temp_dir, \"clients_model.pt\")\n",
    "                )\n",
    "                self.logger.log(\"Find existed clients model...\")\n",
    "        else:\n",
    "            self.logger.log(\"Initializing clients model...\")\n",
    "            \n",
    "            self.client_model_params_list = [\n",
    "#                 list(self.backbone(self.args.dataset).state_dict().values())\n",
    "                list(self.backbone().state_dict().values())\n",
    "                for _ in range(self.client_num_in_total)\n",
    "            ]\n",
    "            \n",
    "#         print(\"client_model_params_list:: Pserver \",self.client_model_params_list)\n",
    "            \n",
    "        _dummy_model = self.backbone()\n",
    "        \n",
    "          \n",
    "        self.hypernet = HyperNetwork(\n",
    "            embedding_dim=self.args.embedding_dim,\n",
    "            client_num=self.client_num_in_total,\n",
    "            hidden_dim=self.args.hidden_dim,\n",
    "            backbone=_dummy_model,\n",
    "            K=self.args.k,\n",
    "            gpu=self.args.gpu,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        self.trainer = pFedLAClient(\n",
    "            backbone=_dummy_model,\n",
    "            dataset=self.args.dataset,\n",
    "            batch_size=self.args.batch_size,\n",
    "            valset_ratio=self.args.testset_ratio,\n",
    "            testset_ratio=self.args.testset_ratio,\n",
    "            local_epochs=self.args.local_epochs,\n",
    "            local_lr=self.args.local_lr,\n",
    "            logger=0,\n",
    "            gpu=self.args.gpu,\n",
    "        )\n",
    "        \n",
    "        self.all_params_name = [name for name in _dummy_model.state_dict().keys()]     \n",
    "        \n",
    "#         self.all_params_name=_dummy_model\n",
    "        \n",
    "        self.trainable_params_name = [\n",
    "                name\n",
    "                for name, param in _dummy_model.state_dict(keep_vars=True).items()\n",
    "                if param.requires_grad\n",
    "            ]\n",
    "        \n",
    "        \n",
    "    def train(self) -> None:\n",
    "#         print(\"PfedL server Train called\")\n",
    "        self.logger.log(\"=\" * 30, \"TRAINING\", \"=\" * 30, style=\"bold green\")\n",
    "        progress_bar = (\n",
    "            track(\n",
    "                range(self.global_epochs),\n",
    "                \"[bold green]Training...\",\n",
    "                console=self.logger,\n",
    "            )\n",
    "            if not self.args.log\n",
    "            else tqdm(range(self.global_epochs), \"Training...\")\n",
    "        )        \n",
    "        \n",
    "        for E in progress_bar:\n",
    "\n",
    "            if E % self.args.verbose_gap == 0:\n",
    "                self.logger.log(\"=\" * 30, f\"ROUND: {E}\", \"=\" * 30)\n",
    "\n",
    "            selected_clients = random.sample(\n",
    "                self.client_id_indices, self.args.client_num_per_round\n",
    "            )\n",
    "            print(\"selected_clients\",selected_clients)\n",
    "            selected_clients=[0,1,2,3]\n",
    "            \n",
    "            print(\"self.global_epochs:: E, progress_bar \", E,\" :: \",progress_bar)\n",
    "            \n",
    "            for client_id in selected_clients:\n",
    "                (\n",
    "                    client_local_params,\n",
    "                    retain_blocks,\n",
    "                ) = self.generate_client_model_parameters(client_id)\n",
    "                \n",
    "#                 print(\"client_local_params:: \\n\",client_local_params)\n",
    "                \n",
    "                diff, stats = self.trainer.train(\n",
    "                    client_id=client_id,\n",
    "                    model_params=client_local_params,\n",
    "                    verbose=(E % self.args.verbose_gap) == 1,\n",
    "                )\n",
    "                \n",
    "                self.all_clients_stats[client_id][f\"ROUND: {E}\"] = (\n",
    "                    f\"retain {retain_blocks}, {stats['loss_before']:.4f} -> {stats['loss_after']:.4f}\",\n",
    "                )                            \n",
    "\n",
    "                self.update_hypernetwork(client_id, diff, retain_blocks)\n",
    "\n",
    "                self.update_client_model_parameters(client_id, diff)\n",
    "                \n",
    "                self.tracker_diff.append(diff)\n",
    "            \n",
    "            if E % self.args.save_period == 0:\n",
    "                torch.save(self.client_model_params_list, os.path.join(self.temp_dir, \"clients_model.pt\"))\n",
    "                with open(os.path.join(self.temp_dir, \"epoch.pkl\"), \"wb\") as f:\n",
    "                    pickle.dump(E, f)\n",
    "                    \n",
    "        self.logger.log(self.all_clients_stats)\n",
    "    \n",
    "    \n",
    "    def test(self) -> None:\n",
    "        \n",
    "        print(\"------testing------\")\n",
    "        self.logger.log(\"=\" * 30, \"TESTING\", \"=\" * 30, style=\"bold blue\")\n",
    "        all_loss = []\n",
    "        all_acc = []\n",
    "        for client_id in track(\n",
    "            self.client_id_indices,\n",
    "            \"[bold blue]Testing...\",\n",
    "            console=self.logger,\n",
    "            disable=self.args.log,\n",
    "        ):\n",
    "            client_local_params, retain_blocks = self.generate_client_model_parameters(\n",
    "                client_id\n",
    "            )\n",
    "            dummy_diff, stats = self.trainer.test(\n",
    "                client_id=client_id, model_params=client_local_params,\n",
    "            )\n",
    "\n",
    "            # NOTE: make sure that all client model params are on CPU, not CUDA\n",
    "            # or self.generate_...() would raise the error of stacking tensors on different devices\n",
    "            self.update_client_model_parameters(client_id, dummy_diff)\n",
    "            self.logger.log(\n",
    "                f\"client [{client_id}] retain {retain_blocks}, [red]loss: {stats['loss']:.4f}    [magenta]accuracy: {stats['acc']:.2f}%\"\n",
    "            )\n",
    "            all_loss.append(stats[\"loss\"])\n",
    "            all_acc.append(stats[\"acc\"])\n",
    "\n",
    "        self.logger.log(\"=\" * 20, \"RESULTS\", \"=\" * 20, style=\"bold green\")\n",
    "        self.logger.log(\n",
    "            \"loss: {:.4f}    accuracy: {:.2f}%\".format(\n",
    "                sum(all_loss) / len(all_loss), sum(all_acc) / len(all_acc),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def update_client_model_parameters(\n",
    "        self, client_id: int, delta: OrderedDict[str, torch.Tensor],\n",
    "    ) -> None:\n",
    "        updated_params = []\n",
    "        for param, diff in zip(\n",
    "            self.client_model_params_list[client_id], delta.values()\n",
    "        ):\n",
    "            updated_params.append((param + diff).detach().cpu())\n",
    "        self.client_model_params_list[client_id] = updated_params\n",
    "\n",
    "    \n",
    "    def generate_client_model_parameters(\n",
    "            self, client_id: int\n",
    "        ) -> Tuple[OrderedDict[str, torch.Tensor], List[str]]:\n",
    "            layer_params_dict = dict(\n",
    "                zip(self.all_params_name, list(zip(*self.client_model_params_list)))\n",
    "            )\n",
    "            alpha, retain_blocks = self.hypernet(client_id)\n",
    "            aggregated_parameters = {}\n",
    "            default_weight = torch.tensor(\n",
    "                [i == client_id for i in range(self.client_num_in_total)],\n",
    "                dtype=torch.float,\n",
    "                device=self.device,\n",
    "            )\n",
    "            for name in self.all_params_name:\n",
    "                if name in self.trainable_params_name:\n",
    "                    a = alpha[name.split(\".\")[0]]\n",
    "                else:\n",
    "                    a = default_weight\n",
    "                if a.sum() == 0:\n",
    "                    self.logger.log(self.all_clients_stats)\n",
    "                    raise RuntimeError(\n",
    "                        f\"client [{client_id}]'s {name.split('.')[0]} alpha is a all 0 vector\"\n",
    "                    )\n",
    "                    \n",
    "                self.tracker_temp=a\n",
    "                \n",
    "                aggregated_parameters[name] = torch.sum(\n",
    "                    a\n",
    "                    / a.sum()\n",
    "                    * torch.stack(layer_params_dict[name], dim=-1).to(self.device),\n",
    "                    dim=-1,\n",
    "                )\n",
    "\n",
    "            self.client_model_params_list[client_id] = list(aggregated_parameters.values())\n",
    "            \n",
    "            return aggregated_parameters, retain_blocks\n",
    "    \n",
    "    def update_hypernetwork(\n",
    "        self,\n",
    "        client_id: int,\n",
    "        diff: OrderedDict[str, torch.Tensor],\n",
    "        retain_blocks: List[str] = [],\n",
    "    ) -> None:\n",
    "        \n",
    "        # calculate gradients\n",
    "        hn_grads = torch.autograd.grad(\n",
    "            outputs=list(\n",
    "                filter(\n",
    "                    lambda param: param.requires_grad,\n",
    "                    self.client_model_params_list[client_id],\n",
    "                )\n",
    "            ),\n",
    "            inputs=self.hypernet.mlp_parameters()\n",
    "            + self.hypernet.fc_layer_parameters()\n",
    "            + self.hypernet.emd_parameters(),\n",
    "\n",
    "            grad_outputs=list(\n",
    "                map(\n",
    "                    lambda tup: tup[1],\n",
    "                    filter(\n",
    "                        lambda tup: tup[1].requires_grad\n",
    "                        and tup[0].split(\".\")[0] not in retain_blocks,\n",
    "                        diff.items(),\n",
    "                    ),\n",
    "                )\n",
    "            ),\n",
    "            allow_unused=True,\n",
    "        )\n",
    "        \n",
    "        mlp_grads = hn_grads[: len(self.hypernet.mlp_parameters())]\n",
    "        fc_grads = hn_grads[\n",
    "            len(self.hypernet.mlp_parameters()) : len(\n",
    "                self.hypernet.mlp_parameters() + self.hypernet.fc_layer_parameters()\n",
    "            )\n",
    "        ]\n",
    "        emd_grads = hn_grads[\n",
    "            len(self.hypernet.mlp_parameters() + self.hypernet.fc_layer_parameters()) :\n",
    "        ]\n",
    "\n",
    "        for param, grad in zip(self.hypernet.fc_layer_parameters(), fc_grads):\n",
    "            if grad is not None:\n",
    "                param.data -= self.args.hn_lr * grad\n",
    "\n",
    "        for param, grad in zip(self.hypernet.mlp_parameters(), mlp_grads):\n",
    "            param.data -= self.args.hn_lr * grad\n",
    "\n",
    "        for param, grad in zip(self.hypernet.emd_parameters(), emd_grads):\n",
    "            param.data -= self.args.hn_lr * grad\n",
    "\n",
    "        self.hypernet.save_hn()\n",
    "        \n",
    "        \n",
    "    def run(self):\n",
    "        print(\"PfedL server RUN called\")        \n",
    "        super().run()\n",
    "        # clean out all HNs \n",
    "        # self.hypernet.clean_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb68405d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e37d8c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.backbone <class '__main__.DeepNet'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Backbone: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DeepNet</span><span style=\"font-weight: bold\">(</span>                                                                                                 \n",
       "  <span style=\"font-weight: bold\">(</span>layer1<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>                                                    \n",
       "  <span style=\"font-weight: bold\">(</span>act1<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ReLU</span><span style=\"font-weight: bold\">()</span>                                                                                                   \n",
       "  <span style=\"font-weight: bold\">(</span>dropout1<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dropout</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">p</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5</span>, <span style=\"color: #808000; text-decoration-color: #808000\">inplace</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>                                                                        \n",
       "  <span style=\"font-weight: bold\">(</span>layer2<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>                                                   \n",
       "  <span style=\"font-weight: bold\">(</span>act2<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ReLU</span><span style=\"font-weight: bold\">()</span>                                                                                                   \n",
       "  <span style=\"font-weight: bold\">(</span>layer3<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">60</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>                                                    \n",
       "  <span style=\"font-weight: bold\">(</span>act3<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ReLU</span><span style=\"font-weight: bold\">()</span>                                                                                                   \n",
       "  <span style=\"font-weight: bold\">(</span>output<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">60</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>                                                      \n",
       "  <span style=\"font-weight: bold\">(</span>sigmoid<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Sigmoid</span><span style=\"font-weight: bold\">()</span>                                                                                             \n",
       "<span style=\"font-weight: bold\">)</span>                                                                                                                  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Backbone: \u001b[1;35mDeepNet\u001b[0m\u001b[1m(\u001b[0m                                                                                                 \n",
       "  \u001b[1m(\u001b[0mlayer1\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m14\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m512\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m                                                    \n",
       "  \u001b[1m(\u001b[0mact1\u001b[1m)\u001b[0m: \u001b[1;35mReLU\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m                                                                                                   \n",
       "  \u001b[1m(\u001b[0mdropout1\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.5\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m                                                                        \n",
       "  \u001b[1m(\u001b[0mlayer2\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m512\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m256\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m                                                   \n",
       "  \u001b[1m(\u001b[0mact2\u001b[1m)\u001b[0m: \u001b[1;35mReLU\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m                                                                                                   \n",
       "  \u001b[1m(\u001b[0mlayer3\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m256\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m60\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m                                                    \n",
       "  \u001b[1m(\u001b[0mact3\u001b[1m)\u001b[0m: \u001b[1;35mReLU\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m                                                                                                   \n",
       "  \u001b[1m(\u001b[0moutput\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m60\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m1\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m                                                      \n",
       "  \u001b[1m(\u001b[0msigmoid\u001b[1m)\u001b[0m: \u001b[1;35mSigmoid\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m                                                                                             \n",
       "\u001b[1m)\u001b[0m                                                                                                                  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Server Called !\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initializing clients model<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                                                                      \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initializing clients model\u001b[33m...\u001b[0m                                                                                      \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Client constructor\n",
      "PFedLa Client constructor\n",
      "PfedL server RUN called\n",
      "Base server Run called\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Arguments:                                                                                                         \n",
       "<span style=\"font-weight: bold\">{</span>                                                                                                                  \n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'k'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>,                                                                                                        \n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'global_epochs'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>,                                                                                            \n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'local_epochs'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>,                                                                                             \n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'local_lr'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.001</span>,                                                                                             \n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'hn_lr'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.002</span>,                                                                                                \n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'verbose_gap'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span>,                                                                                             \n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'embedding_dim'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">200</span>,                                                                                          \n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'hidden_dim'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">200</span>,                                                                                             \n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'dataset'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'no'</span>,                                                                                               \n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'batch_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1000</span>,                                                                                            \n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'valset_ratio'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>,                                                                                           \n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'testset_ratio'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3</span>,                                                                                          \n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'gpu'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,                                                                                                   \n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'log'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,                                                                                                      \n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'seed'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>,                                                                                                     \n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'client_num_per_round'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>,                                                                                     \n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'save_period'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>                                                                                              \n",
       "<span style=\"font-weight: bold\">}</span>                                                                                                                  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Arguments:                                                                                                         \n",
       "\u001b[1m{\u001b[0m                                                                                                                  \n",
       "    \u001b[32m'k'\u001b[0m: \u001b[1;36m2\u001b[0m,                                                                                                        \n",
       "    \u001b[32m'global_epochs'\u001b[0m: \u001b[1;36m3\u001b[0m,                                                                                            \n",
       "    \u001b[32m'local_epochs'\u001b[0m: \u001b[1;36m5\u001b[0m,                                                                                             \n",
       "    \u001b[32m'local_lr'\u001b[0m: \u001b[1;36m0.001\u001b[0m,                                                                                             \n",
       "    \u001b[32m'hn_lr'\u001b[0m: \u001b[1;36m0.002\u001b[0m,                                                                                                \n",
       "    \u001b[32m'verbose_gap'\u001b[0m: \u001b[1;36m20\u001b[0m,                                                                                             \n",
       "    \u001b[32m'embedding_dim'\u001b[0m: \u001b[1;36m200\u001b[0m,                                                                                          \n",
       "    \u001b[32m'hidden_dim'\u001b[0m: \u001b[1;36m200\u001b[0m,                                                                                             \n",
       "    \u001b[32m'dataset'\u001b[0m: \u001b[32m'no'\u001b[0m,                                                                                               \n",
       "    \u001b[32m'batch_size'\u001b[0m: \u001b[1;36m1000\u001b[0m,                                                                                            \n",
       "    \u001b[32m'valset_ratio'\u001b[0m: \u001b[1;36m0.0\u001b[0m,                                                                                           \n",
       "    \u001b[32m'testset_ratio'\u001b[0m: \u001b[1;36m0.3\u001b[0m,                                                                                          \n",
       "    \u001b[32m'gpu'\u001b[0m: \u001b[3;92mTrue\u001b[0m,                                                                                                   \n",
       "    \u001b[32m'log'\u001b[0m: \u001b[1;36m0\u001b[0m,                                                                                                      \n",
       "    \u001b[32m'seed'\u001b[0m: \u001b[1;36m5\u001b[0m,                                                                                                     \n",
       "    \u001b[32m'client_num_per_round'\u001b[0m: \u001b[1;36m2\u001b[0m,                                                                                     \n",
       "    \u001b[32m'save_period'\u001b[0m: \u001b[1;36m10\u001b[0m                                                                                              \n",
       "\u001b[1m}\u001b[0m                                                                                                                  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">============================== TRAINING ==============================                                             </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m============================== TRAINING ==============================                                             \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dbcb46c76d94dae874ef3386c2678f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">============================== ROUND: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> ==============================                                             \n",
       "</pre>\n"
      ],
      "text/plain": [
       "============================== ROUND: \u001b[1;36m0\u001b[0m ==============================                                             \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">selected_clients [2, 3]\n",
       "</pre>\n"
      ],
      "text/plain": [
       "selected_clients [2, 3]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">self.global_epochs:: E, progress_bar  0  ::  &lt;generator object track at 0x7f628ce2b5a0&gt;\n",
       "</pre>\n"
      ],
      "text/plain": [
       "self.global_epochs:: E, progress_bar  0  ::  <generator object track at 0x7f628ce2b5a0>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">self.client_id::  0\n",
       "</pre>\n"
      ],
      "text/plain": [
       "self.client_id::  0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Testing Accuracy: 64.44%\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Testing Accuracy: 64.44%\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">0\n",
       "</pre>\n"
      ],
      "text/plain": [
       "0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">1\n",
       "</pre>\n"
      ],
      "text/plain": [
       "1\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">2\n",
       "</pre>\n"
      ],
      "text/plain": [
       "2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">3\n",
       "</pre>\n"
      ],
      "text/plain": [
       "3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">4\n",
       "</pre>\n"
      ],
      "text/plain": [
       "4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Testing Accuracy: 64.44%\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Testing Accuracy: 64.44%\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">self.client_id::  1\n",
       "</pre>\n"
      ],
      "text/plain": [
       "self.client_id::  1\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Testing Accuracy: 37.69%\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Testing Accuracy: 37.69%\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">0\n",
       "</pre>\n"
      ],
      "text/plain": [
       "0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">1\n",
       "</pre>\n"
      ],
      "text/plain": [
       "1\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">2\n",
       "</pre>\n"
      ],
      "text/plain": [
       "2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">3\n",
       "</pre>\n"
      ],
      "text/plain": [
       "3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">4\n",
       "</pre>\n"
      ],
      "text/plain": [
       "4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Testing Accuracy: 37.69%\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Testing Accuracy: 37.69%\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">self.client_id::  2\n",
       "</pre>\n"
      ],
      "text/plain": [
       "self.client_id::  2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Testing Accuracy: 52.87%\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Testing Accuracy: 52.87%\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">0\n",
       "</pre>\n"
      ],
      "text/plain": [
       "0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">1\n",
       "</pre>\n"
      ],
      "text/plain": [
       "1\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">2\n",
       "</pre>\n"
      ],
      "text/plain": [
       "2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">3\n",
       "</pre>\n"
      ],
      "text/plain": [
       "3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">4\n",
       "</pre>\n"
      ],
      "text/plain": [
       "4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Testing Accuracy: 52.87%\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Testing Accuracy: 52.87%\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">self.client_id::  3\n",
       "</pre>\n"
      ],
      "text/plain": [
       "self.client_id::  3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Testing Accuracy: 62.10%\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Testing Accuracy: 62.10%\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">0\n",
       "</pre>\n"
      ],
      "text/plain": [
       "0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">1\n",
       "</pre>\n"
      ],
      "text/plain": [
       "1\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">2\n",
       "</pre>\n"
      ],
      "text/plain": [
       "2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">3\n",
       "</pre>\n"
      ],
      "text/plain": [
       "3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">4\n",
       "</pre>\n"
      ],
      "text/plain": [
       "4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Testing Accuracy: 62.10%\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Testing Accuracy: 62.10%\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">selected_clients [2, 3]\n",
       "</pre>\n"
      ],
      "text/plain": [
       "selected_clients [2, 3]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">self.global_epochs:: E, progress_bar  1  ::  &lt;generator object track at 0x7f628ce2b5a0&gt;\n",
       "</pre>\n"
      ],
      "text/plain": [
       "self.global_epochs:: E, progress_bar  1  ::  <generator object track at 0x7f628ce2b5a0>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">self.client_id::  0\n",
       "</pre>\n"
      ],
      "text/plain": [
       "self.client_id::  0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Testing Accuracy: 64.44%\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Testing Accuracy: 64.44%\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">0\n",
       "</pre>\n"
      ],
      "text/plain": [
       "0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">1\n",
       "</pre>\n"
      ],
      "text/plain": [
       "1\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">2\n",
       "</pre>\n"
      ],
      "text/plain": [
       "2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">3\n",
       "</pre>\n"
      ],
      "text/plain": [
       "3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">4\n",
       "</pre>\n"
      ],
      "text/plain": [
       "4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Testing Accuracy: 64.44%\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Testing Accuracy: 64.44%\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">self.client_id::  1\n",
       "</pre>\n"
      ],
      "text/plain": [
       "self.client_id::  1\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Testing Accuracy: 37.69%\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Testing Accuracy: 37.69%\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">0\n",
       "</pre>\n"
      ],
      "text/plain": [
       "0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">1\n",
       "</pre>\n"
      ],
      "text/plain": [
       "1\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">2\n",
       "</pre>\n"
      ],
      "text/plain": [
       "2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">3\n",
       "</pre>\n"
      ],
      "text/plain": [
       "3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">4\n",
       "</pre>\n"
      ],
      "text/plain": [
       "4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Testing Accuracy: 37.69%\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Testing Accuracy: 37.69%\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">self.client_id::  2\n",
       "</pre>\n"
      ],
      "text/plain": [
       "self.client_id::  2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Testing Accuracy: 52.87%\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Testing Accuracy: 52.87%\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">0\n",
       "</pre>\n"
      ],
      "text/plain": [
       "0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">1\n",
       "</pre>\n"
      ],
      "text/plain": [
       "1\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">2\n",
       "</pre>\n"
      ],
      "text/plain": [
       "2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">3\n",
       "</pre>\n"
      ],
      "text/plain": [
       "3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">4\n",
       "</pre>\n"
      ],
      "text/plain": [
       "4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Testing Accuracy: 52.87%\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Testing Accuracy: 52.87%\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">self.client_id::  3\n",
       "</pre>\n"
      ],
      "text/plain": [
       "self.client_id::  3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Testing Accuracy: 62.10%\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Testing Accuracy: 62.10%\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">0\n",
       "</pre>\n"
      ],
      "text/plain": [
       "0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">1\n",
       "</pre>\n"
      ],
      "text/plain": [
       "1\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">2\n",
       "</pre>\n"
      ],
      "text/plain": [
       "2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">3\n",
       "</pre>\n"
      ],
      "text/plain": [
       "3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">4\n",
       "</pre>\n"
      ],
      "text/plain": [
       "4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Testing Accuracy: 62.10%\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Testing Accuracy: 62.10%\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">selected_clients [0, 1]\n",
       "</pre>\n"
      ],
      "text/plain": [
       "selected_clients [0, 1]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">self.global_epochs:: E, progress_bar  2  ::  &lt;generator object track at 0x7f628ce2b5a0&gt;\n",
       "</pre>\n"
      ],
      "text/plain": [
       "self.global_epochs:: E, progress_bar  2  ::  <generator object track at 0x7f628ce2b5a0>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">self.client_id::  0\n",
       "</pre>\n"
      ],
      "text/plain": [
       "self.client_id::  0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Testing Accuracy: 64.44%\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Testing Accuracy: 64.44%\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">0\n",
       "</pre>\n"
      ],
      "text/plain": [
       "0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">1\n",
       "</pre>\n"
      ],
      "text/plain": [
       "1\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">2\n",
       "</pre>\n"
      ],
      "text/plain": [
       "2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">3\n",
       "</pre>\n"
      ],
      "text/plain": [
       "3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">4\n",
       "</pre>\n"
      ],
      "text/plain": [
       "4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Testing Accuracy: 64.44%\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Testing Accuracy: 64.44%\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">self.client_id::  1\n",
       "</pre>\n"
      ],
      "text/plain": [
       "self.client_id::  1\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Testing Accuracy: 37.69%\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Testing Accuracy: 37.69%\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">0\n",
       "</pre>\n"
      ],
      "text/plain": [
       "0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">1\n",
       "</pre>\n"
      ],
      "text/plain": [
       "1\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">2\n",
       "</pre>\n"
      ],
      "text/plain": [
       "2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">3\n",
       "</pre>\n"
      ],
      "text/plain": [
       "3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">4\n",
       "</pre>\n"
      ],
      "text/plain": [
       "4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Testing Accuracy: 37.69%\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Testing Accuracy: 37.69%\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">self.client_id::  2\n",
       "</pre>\n"
      ],
      "text/plain": [
       "self.client_id::  2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Testing Accuracy: 52.87%\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Testing Accuracy: 52.87%\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">0\n",
       "</pre>\n"
      ],
      "text/plain": [
       "0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">1\n",
       "</pre>\n"
      ],
      "text/plain": [
       "1\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">2\n",
       "</pre>\n"
      ],
      "text/plain": [
       "2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">3\n",
       "</pre>\n"
      ],
      "text/plain": [
       "3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">4\n",
       "</pre>\n"
      ],
      "text/plain": [
       "4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Testing Accuracy: 52.87%\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Testing Accuracy: 52.87%\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">self.client_id::  3\n",
       "</pre>\n"
      ],
      "text/plain": [
       "self.client_id::  3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Testing Accuracy: 62.10%\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Testing Accuracy: 62.10%\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">0\n",
       "</pre>\n"
      ],
      "text/plain": [
       "0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">1\n",
       "</pre>\n"
      ],
      "text/plain": [
       "1\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">2\n",
       "</pre>\n"
      ],
      "text/plain": [
       "2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">3\n",
       "</pre>\n"
      ],
      "text/plain": [
       "3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">4\n",
       "</pre>\n"
      ],
      "text/plain": [
       "4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Testing Accuracy: 62.10%\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Testing Accuracy: 62.10%\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>                                                                                                                  \n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>: <span style=\"font-weight: bold\">{</span>                                                                                                           \n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'ROUND: 0'</span>: <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"retain ['layer3', 'layer2'], 0.6902 -&gt; 0.6902\"</span>,<span style=\"font-weight: bold\">)</span>,                                            \n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'ROUND: 1'</span>: <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"retain ['layer3', 'layer2'], 0.6888 -&gt; 0.6888\"</span>,<span style=\"font-weight: bold\">)</span>,                                            \n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'ROUND: 2'</span>: <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"retain ['layer3', 'layer2'], 0.6886 -&gt; 0.6886\"</span>,<span style=\"font-weight: bold\">)</span>                                             \n",
       "    <span style=\"font-weight: bold\">}</span>,                                                                                                             \n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: <span style=\"font-weight: bold\">{</span>                                                                                                           \n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'ROUND: 0'</span>: <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"retain ['output', 'layer3'], 0.6943 -&gt; 0.6943\"</span>,<span style=\"font-weight: bold\">)</span>,                                            \n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'ROUND: 1'</span>: <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"retain ['output', 'layer3'], 0.6944 -&gt; 0.6944\"</span>,<span style=\"font-weight: bold\">)</span>,                                            \n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'ROUND: 2'</span>: <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"retain ['output', 'layer3'], 0.6945 -&gt; 0.6945\"</span>,<span style=\"font-weight: bold\">)</span>                                             \n",
       "    <span style=\"font-weight: bold\">}</span>,                                                                                                             \n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>: <span style=\"font-weight: bold\">{</span>                                                                                                           \n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'ROUND: 0'</span>: <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"retain ['layer3', 'layer1'], 0.6924 -&gt; 0.6924\"</span>,<span style=\"font-weight: bold\">)</span>,                                            \n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'ROUND: 1'</span>: <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"retain ['layer3', 'layer1'], 0.6924 -&gt; 0.6924\"</span>,<span style=\"font-weight: bold\">)</span>,                                            \n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'ROUND: 2'</span>: <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"retain ['layer3', 'layer1'], 0.6924 -&gt; 0.6924\"</span>,<span style=\"font-weight: bold\">)</span>                                             \n",
       "    <span style=\"font-weight: bold\">}</span>,                                                                                                             \n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>: <span style=\"font-weight: bold\">{</span>                                                                                                           \n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'ROUND: 0'</span>: <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"retain ['output', 'layer1'], 0.6845 -&gt; 0.6845\"</span>,<span style=\"font-weight: bold\">)</span>,                                            \n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'ROUND: 1'</span>: <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"retain ['output', 'layer1'], 0.6848 -&gt; 0.6848\"</span>,<span style=\"font-weight: bold\">)</span>,                                            \n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'ROUND: 2'</span>: <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"retain ['output', 'layer1'], 0.6849 -&gt; 0.6849\"</span>,<span style=\"font-weight: bold\">)</span>                                             \n",
       "    <span style=\"font-weight: bold\">}</span>                                                                                                              \n",
       "<span style=\"font-weight: bold\">}</span>                                                                                                                  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m                                                                                                                  \n",
       "    \u001b[1;36m0\u001b[0m: \u001b[1m{\u001b[0m                                                                                                           \n",
       "        \u001b[32m'ROUND: 0'\u001b[0m: \u001b[1m(\u001b[0m\u001b[32m\"retain \u001b[0m\u001b[32m[\u001b[0m\u001b[32m'layer3', 'layer2'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, 0.6902 -> 0.6902\"\u001b[0m,\u001b[1m)\u001b[0m,                                            \n",
       "        \u001b[32m'ROUND: 1'\u001b[0m: \u001b[1m(\u001b[0m\u001b[32m\"retain \u001b[0m\u001b[32m[\u001b[0m\u001b[32m'layer3', 'layer2'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, 0.6888 -> 0.6888\"\u001b[0m,\u001b[1m)\u001b[0m,                                            \n",
       "        \u001b[32m'ROUND: 2'\u001b[0m: \u001b[1m(\u001b[0m\u001b[32m\"retain \u001b[0m\u001b[32m[\u001b[0m\u001b[32m'layer3', 'layer2'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, 0.6886 -> 0.6886\"\u001b[0m,\u001b[1m)\u001b[0m                                             \n",
       "    \u001b[1m}\u001b[0m,                                                                                                             \n",
       "    \u001b[1;36m1\u001b[0m: \u001b[1m{\u001b[0m                                                                                                           \n",
       "        \u001b[32m'ROUND: 0'\u001b[0m: \u001b[1m(\u001b[0m\u001b[32m\"retain \u001b[0m\u001b[32m[\u001b[0m\u001b[32m'output', 'layer3'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, 0.6943 -> 0.6943\"\u001b[0m,\u001b[1m)\u001b[0m,                                            \n",
       "        \u001b[32m'ROUND: 1'\u001b[0m: \u001b[1m(\u001b[0m\u001b[32m\"retain \u001b[0m\u001b[32m[\u001b[0m\u001b[32m'output', 'layer3'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, 0.6944 -> 0.6944\"\u001b[0m,\u001b[1m)\u001b[0m,                                            \n",
       "        \u001b[32m'ROUND: 2'\u001b[0m: \u001b[1m(\u001b[0m\u001b[32m\"retain \u001b[0m\u001b[32m[\u001b[0m\u001b[32m'output', 'layer3'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, 0.6945 -> 0.6945\"\u001b[0m,\u001b[1m)\u001b[0m                                             \n",
       "    \u001b[1m}\u001b[0m,                                                                                                             \n",
       "    \u001b[1;36m2\u001b[0m: \u001b[1m{\u001b[0m                                                                                                           \n",
       "        \u001b[32m'ROUND: 0'\u001b[0m: \u001b[1m(\u001b[0m\u001b[32m\"retain \u001b[0m\u001b[32m[\u001b[0m\u001b[32m'layer3', 'layer1'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, 0.6924 -> 0.6924\"\u001b[0m,\u001b[1m)\u001b[0m,                                            \n",
       "        \u001b[32m'ROUND: 1'\u001b[0m: \u001b[1m(\u001b[0m\u001b[32m\"retain \u001b[0m\u001b[32m[\u001b[0m\u001b[32m'layer3', 'layer1'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, 0.6924 -> 0.6924\"\u001b[0m,\u001b[1m)\u001b[0m,                                            \n",
       "        \u001b[32m'ROUND: 2'\u001b[0m: \u001b[1m(\u001b[0m\u001b[32m\"retain \u001b[0m\u001b[32m[\u001b[0m\u001b[32m'layer3', 'layer1'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, 0.6924 -> 0.6924\"\u001b[0m,\u001b[1m)\u001b[0m                                             \n",
       "    \u001b[1m}\u001b[0m,                                                                                                             \n",
       "    \u001b[1;36m3\u001b[0m: \u001b[1m{\u001b[0m                                                                                                           \n",
       "        \u001b[32m'ROUND: 0'\u001b[0m: \u001b[1m(\u001b[0m\u001b[32m\"retain \u001b[0m\u001b[32m[\u001b[0m\u001b[32m'output', 'layer1'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, 0.6845 -> 0.6845\"\u001b[0m,\u001b[1m)\u001b[0m,                                            \n",
       "        \u001b[32m'ROUND: 1'\u001b[0m: \u001b[1m(\u001b[0m\u001b[32m\"retain \u001b[0m\u001b[32m[\u001b[0m\u001b[32m'output', 'layer1'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, 0.6848 -> 0.6848\"\u001b[0m,\u001b[1m)\u001b[0m,                                            \n",
       "        \u001b[32m'ROUND: 2'\u001b[0m: \u001b[1m(\u001b[0m\u001b[32m\"retain \u001b[0m\u001b[32m[\u001b[0m\u001b[32m'output', 'layer1'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, 0.6849 -> 0.6849\"\u001b[0m,\u001b[1m)\u001b[0m                                             \n",
       "    \u001b[1m}\u001b[0m                                                                                                              \n",
       "\u001b[1m}\u001b[0m                                                                                                                  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------testing------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">============================== TESTING ==============================                                              </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34m============================== TESTING ==============================                                              \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "413cd0c2d6504c8db73ccd567a13f052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">self.client_id::  0\n",
       "</pre>\n"
      ],
      "text/plain": [
       "self.client_id::  0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Testing Accuracy: 64.44%\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Testing Accuracy: 64.44%\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">client <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span> retain <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'layer3'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'layer2'</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #800000; text-decoration-color: #800000\">loss: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.6886</span><span style=\"color: #800000; text-decoration-color: #800000\">    </span><span style=\"color: #800080; text-decoration-color: #800080\">accuracy: </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">0.64</span><span style=\"color: #800080; text-decoration-color: #800080\">%</span>                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "client \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m retain \u001b[1m[\u001b[0m\u001b[32m'layer3'\u001b[0m, \u001b[32m'layer2'\u001b[0m\u001b[1m]\u001b[0m, \u001b[31mloss: \u001b[0m\u001b[1;31m0.6886\u001b[0m\u001b[31m    \u001b[0m\u001b[35maccuracy: \u001b[0m\u001b[1;35m0.64\u001b[0m\u001b[35m%\u001b[0m                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">self.client_id::  1\n",
       "</pre>\n"
      ],
      "text/plain": [
       "self.client_id::  1\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Testing Accuracy: 37.69%\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Testing Accuracy: 37.69%\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">client <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span> retain <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'output'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'layer3'</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #800000; text-decoration-color: #800000\">loss: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.6946</span><span style=\"color: #800000; text-decoration-color: #800000\">    </span><span style=\"color: #800080; text-decoration-color: #800080\">accuracy: </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">0.38</span><span style=\"color: #800080; text-decoration-color: #800080\">%</span>                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "client \u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m retain \u001b[1m[\u001b[0m\u001b[32m'output'\u001b[0m, \u001b[32m'layer3'\u001b[0m\u001b[1m]\u001b[0m, \u001b[31mloss: \u001b[0m\u001b[1;31m0.6946\u001b[0m\u001b[31m    \u001b[0m\u001b[35maccuracy: \u001b[0m\u001b[1;35m0.38\u001b[0m\u001b[35m%\u001b[0m                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">self.client_id::  2\n",
       "</pre>\n"
      ],
      "text/plain": [
       "self.client_id::  2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Testing Accuracy: 52.87%\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Testing Accuracy: 52.87%\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">client <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">]</span> retain <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'layer3'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'layer1'</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #800000; text-decoration-color: #800000\">loss: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.6924</span><span style=\"color: #800000; text-decoration-color: #800000\">    </span><span style=\"color: #800080; text-decoration-color: #800080\">accuracy: </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">0.53</span><span style=\"color: #800080; text-decoration-color: #800080\">%</span>                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "client \u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m retain \u001b[1m[\u001b[0m\u001b[32m'layer3'\u001b[0m, \u001b[32m'layer1'\u001b[0m\u001b[1m]\u001b[0m, \u001b[31mloss: \u001b[0m\u001b[1;31m0.6924\u001b[0m\u001b[31m    \u001b[0m\u001b[35maccuracy: \u001b[0m\u001b[1;35m0.53\u001b[0m\u001b[35m%\u001b[0m                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">self.client_id::  3\n",
       "</pre>\n"
      ],
      "text/plain": [
       "self.client_id::  3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Testing Accuracy: 62.10%\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Testing Accuracy: 62.10%\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">client <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">]</span> retain <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'output'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'layer1'</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #800000; text-decoration-color: #800000\">loss: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.6850</span><span style=\"color: #800000; text-decoration-color: #800000\">    </span><span style=\"color: #800080; text-decoration-color: #800080\">accuracy: </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">0.62</span><span style=\"color: #800080; text-decoration-color: #800080\">%</span>                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "client \u001b[1m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m]\u001b[0m retain \u001b[1m[\u001b[0m\u001b[32m'output'\u001b[0m, \u001b[32m'layer1'\u001b[0m\u001b[1m]\u001b[0m, \u001b[31mloss: \u001b[0m\u001b[1;31m0.6850\u001b[0m\u001b[31m    \u001b[0m\u001b[35maccuracy: \u001b[0m\u001b[1;35m0.62\u001b[0m\u001b[35m%\u001b[0m                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">==================== RESULTS ====================                                                                  </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m==================== RESULTS ====================                                                                  \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">loss: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6901</span>    accuracy: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.54</span>%                                                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "loss: \u001b[1;36m0.6901\u001b[0m    accuracy: \u001b[1;36m0.54\u001b[0m%                                                                                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "args1 = Namespace(\n",
    "    k=2,\n",
    "    global_epochs=3,\n",
    "    local_epochs=5,\n",
    "    local_lr=0.001,\n",
    "    hn_lr=0.002,\n",
    "    verbose_gap=20,\n",
    "    embedding_dim=200,\n",
    "    hidden_dim=200,\n",
    "    dataset=\"no\",\n",
    "    batch_size=1000,\n",
    "    valset_ratio=0.0,\n",
    "    testset_ratio=0.3,\n",
    "    gpu=True,\n",
    "    log=0,\n",
    "    seed=5,\n",
    "    client_num_per_round=2,\n",
    "    save_period=10,\n",
    ")\n",
    "\n",
    "server = pFedLAServer()\n",
    "server.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f732203",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f11e4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b499095",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e761bc0f-c2b4-4b6e-ae81-5098a4665b83",
   "metadata": {},
   "source": [
    "# Testing of client models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6b7239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 14 : input shape\n",
    "        self.layer1 = nn.Linear(14, 512)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "        self.layer2 = nn.Linear(512, 256)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(256, 60)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.output = nn.Linear(60, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.layer1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.act2(self.layer2(x))\n",
    "        x = self.act3(self.layer3(x))\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "193901aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_clients_model=\"/home/chiragapandav/Downloads/Hiwi/Improving-Fairness-via-Federated-Learning/FedFB/my_model/clients_model.pt\"\n",
    "loaded_client_model_params_list=torch.load(path_to_clients_model) # 4 models params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b7513a-850d-49d7-96e6-a10a5d5f8173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4006c340-a710-438c-b6d9-60dc276dabc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbef3e89-3a3f-4643-a6cb-1d801fe064dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_params(model, params):\n",
    "    state_dict = model.state_dict()\n",
    "    for key in state_dict:\n",
    "        if key in params:\n",
    "            state_dict[key] = params[key]\n",
    "    model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "separate_models = []\n",
    "for params in loaded_client_model_params_list:    \n",
    "    base_client_model = DeepNet()\n",
    "    base_client_model = load_params(base_client_model, params)\n",
    "    separate_models.append(base_client_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fbeee2d7-92b6-41e0-b550-a2a660d93492",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    size = 0\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, targets, sens in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss += criterion(outputs, targets)\n",
    "        predicted = outputs > 0.5\n",
    "\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "    loss = loss / len(test_loader)\n",
    "    acc = correct / total\n",
    "\n",
    "    print(f\"Testing Accuracy: {acc:.2%}\")\n",
    "\n",
    "    return loss, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0022b95c-71a8-4394-9c9a-b6f90e3805a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 42.70%\n"
     ]
    }
   ],
   "source": [
    "test_loader=\"/home/chiragapandav/Downloads/Hiwi/Improving-Fairness-via-Federated-Learning/FedFB/data_fairFed/server_testing_client.pkl\"\n",
    "with open(test_loader, 'rb') as file:\n",
    "    test_loader = pickle.load(file)\n",
    "\n",
    "\n",
    "model_to_test = separate_models[3] \n",
    "model_to_test = model_to_test.to(device)\n",
    "\n",
    "loss, accuracy = evaluate(model_to_test, test_loader[0], criterion, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82afdfc7-f4e0-4a01-8acf-2510f3be70a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaae872-f9c8-4b35-8975-99a558352bd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17efad80-f7f3-44f1-8190-78f45e3e446c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e7e4b1-9e88-4171-bb41-71531708aa18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425428a8-3e2f-45c6-8005-666aceb6ca02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3fd795-3e44-46b1-bf6c-85751ec8bb31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
